Automatic Document Metadata Extraction using Support Vector Machines
Hui Han½ C. Lee Giles½ ¾ Eren Manavoglu½ Hongyuan Zha½
½
Department of Computer Science and Engineering ¾ The School of Information Sciences and Technology
The Pennsylvania State University University Park, PA, 16802
hhan,zha,manavogl @cse.psu.edu giles@ist.psu.edu
Zhenyue Zhang
Department of Mathematics, Zhejiang University
Yu-Quan Campus, Hangzhou 310027, P.R. China
zyzhang@math.zju.edu.cn
Edward A. Fox
Department of Computer Science, Virginia Polytechnic Institute and State University
660 McBryde Hall, M/C 0106, Blacksburg, VA 24061
fox@vt.edu

Abstract
Automatic metadata generation provides scalability and
usability for digital libraries and their collections. Machine learning methods offer robust and adaptable automatic metadata extraction. We describe a Support Vector
Machine classiﬁcation-based method for metadata extraction from header part of research papers and show that it
outperforms other machine learning methods on the same
task. The method ﬁrst classiﬁes each line of the header into
one or more of 15 classes. An iterative convergence procedure is then used to improve the line classiﬁcation by using
the predicted class labels of its neighbor lines in the previous round. Further metadata extraction is done by seeking
the best chunk boundaries of each line. We found that discovery and use of the structural patterns of the data and
domain based word clustering can improve the metadata
extraction performance. An appropriate feature normalization also greatly improves the classiﬁcation performance.
Our metadata extraction method was originally designed
to improve the metadata extraction quality of the digital libraries Citeseer[17] and EbizSearch[24]. We believe it can
be generalized to other digital libraries.

1 Introduction and related work
Interoperability is crucial to the effective use of Digital
Libraries (DL) [19, 23]. The Open Archive Initiatives Protocols for Metadata Harvesting (OAI-PMH) is critical for

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

the process, facilitating the discovery of content stored in
distributed archives [7, 18]. The digital library CITIDEL
(Computing and Information Technology Interactive Digital Educational Library), part of NSDL (National Science
Digital Library), uses OAI-PMH to harvest metadata from
all applicable repositories and provides integrated access
and links across related collections [14]. Support for the
Dublin Core (DC) metadata standard [31] is a requirement
for OAI-PMH compliant archives, while other metadata formats optionally can be transmitted.
However, providing metadata is the responsibility of
each data provider with the quality of the metadata a significant problem. Many data providers [13, 4] have had significant harvesting problems with XML syntax and encoding
issues, even leading to unavailability of service [18]. In fact,
some digital libraries have no metadata to harvest (some
search engines have little or no metadata), or metadata that
is not OAI compliant, e.g., CiteSeer [17]. Non-compliant
metadata must be either automatically wrapped to work
with the OAI protocol, or manually encoded. Building tools
for automatic document metadata extraction and representation will therefore signiﬁcantly improve the amount of
metadata available, the quality of metadata extracted, and
the efﬁciency and speed of the metadata extraction process.
Several methods have been used for automatic metadata
extraction; regular expressions, rule-based parsers, and machine learning are the most popular of these. In general machine learning methods are robust and adaptable and, theoretically, can be used on any document set. Generating the
labeled training data is the rather expensive price that has

to be paid for learning systems. Although regular expressions and rule-based systems do not require any training
and are straightforward to implement, their dependence on
the application domain and the need for an expert to set the
rules or regular expressions causes these methods to have
limited use. Machine learning techniques for information
extraction include symbolic learning, inductive logic programming, grammar induction, Support Vector Machines,
Hidden Markov models, and statistical methods. Hidden
Markov models (HMMs) are the most widely used generative learning method for representing and extracting information from sequential data. However, HMMs are based
on the assumption that features of the model they represent are not independent from each other. Thus, HMMs
have difﬁculty exploiting regularities of a semi-structured
real system. Maximum entropy based Markov models [20]
and conditional random ﬁelds [16] have been introduced to
deal with the problem of independent features.
Recent work by Chieu [5] suggests that the information extraction task also can be addressed as a classiﬁcation
problem. Encouraged by their success in handling high dimensional feature spaces for classiﬁcation problems [12, 9],
we investigate Support Vector Machines (SVMs) for metadata extraction. Related work includes Kudoh et al using the
SVM method for chunk identiﬁcation, Mcnamee et al using
a SVM for named entity extraction [22, 15, 29], and Pasula
et al using relational probability models to solve identity
uncertainty problems [10].
This paper discusses a machine learning method for automatic metadata extraction. The reported extraction results
are based on experiments conducted on research papers.
Most of the directly indexable information (e.g., authors’
names, afﬁliations, addresses, and the title of the paper) are
gathered in the header of a research paper. The header [27]
consists of all the words from the beginning of the paper up
to either the ﬁrst section, usually the introduction, or to the
end of the ﬁrst page, whichever occurs ﬁrst. In the experimental results section we illustrate the dominance of the
introduced SVM-based metadata extraction algorithm over
the well-known HMM based systems [27]. We also introduce a method for extracting individual names from the list
of authors within the same framework and present a new
document metadata extraction method using SVM classiﬁcation, combining chunk identiﬁcation. A new feature extraction method and an iterative line classiﬁcation process
using contextual information also are presented.
The remainder of the paper is organized as follows: section 2 describes the problem and dataset; section 3 presents
our metadata extraction method, together with the cross validation results on 500 training headers; section 4 presents
the experiment result of our metadata extraction algorithm
on the test dataset; section 5 discusses the aspects to be improved and planned future work.

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

2 Problem deﬁnition and dataset
The Dublin Core has been widely used as a metadata
standard and deﬁnes 15 elements for resource description:
Title, Creator, Subject, Description, Contributor, Publisher,
Date, Type, Format, Identiﬁer, Source, Relation, References, Is Referenced By, Language, Rights and Coverage.
However, this is only a basic set of metadata elements and
is used by OAI-PMH for “minimal” interoperability. Extending document metadata through information on both
authors (such as afﬁliation, address, and email), and documents (such as publication number and thesis type), would
provide greater representation power. It also would help in
building uniﬁed services for heterogeneous digital libraries,
while at the same time enabling sophisticated querying of
the databases and facilitating construction of the semantic
web [3]. Seymore et al deﬁned 15 different tags for the
document header [27] to populate the Cora search engine
[21], 4 of which are the same as those in the Dublin Core.
Two of the remaining tags, introduction and end of page, are
functional rather than informative, indicating the end of the
header. Leaving out the functional tags, we adopt their format as extended metatags for research papers. We further
propose to deﬁne afﬁliation as part of the address, instead
of an exclusive tag. Table 1 is a short explanation of the extended metatags and the mapping to Dublin Core metadata
elements.
Figure 1 is an example of meta-tagged document header.
Document metadata extraction also can be viewed as labeling the text with the corresponding metatags. Each metatag
corresponds to a class. Lines 22 and 25 are multi-class lines
containing chunks of information from multiple classes. We
deﬁne a chunk of information as consecutive words that belong to the same class. Line 22 and 25 contain the chunks
of 5 classes: email, web, afﬁliation, address, and note. All
the other lines contain information belonging to one class
only and are therefore called single-class lines.
We use the labeled dataset provided by Seymore et al
[27] to test our method of metadata extraction. The dataset
contains 935 headers of computer science research papers,
with 500 of those belonging to the training set and the remaining 435 headers belonging to the test set. The training
set includes a total of 10025 lines and 23557 word tokens
whereas there are 8904 lines and 20308 word tokens in the
test set. These headers are text ﬁles converted from the pdf
and ps ﬁles. Each line ends with a carriage return and the
line break marks ·Ä· are provided by the dataset for identiﬁcation.
The document headers are semi-structured. We observe
that among total 10025 lines from 500 training headers,
the majority (9775 lines, 97.51%) are single-class lines and
only 250 (2.49%) lines are multi-class lines. Even after
removing the abstract section which is mostly single-class

Table 1. Extended metatags and their mapping to Dublin Core metadata elements
Extended
Metatag
Title
Author

DC Element
Title
Creator

Afﬁliation
Address
Note
Email
Date
Abstract
Introduction
Phone
Keyword
Web
Degree
Pubnum
Page

Description

Subject

Explanation
Title of the paper
The name(s) of the author(s)
of the document
Author’s afﬁliation
Author’s address
Phrases about acknowledgment,
copyright, notices, and citations
Author’s email address
Publication date
An account of the content
Introduction part in the paper
Author’s phone number
The topic of the content of
the document
URL of Author’s webpage
of the document
Language associated with thesis
degree
Publication number
of the document
The end of the page

lines, multi-class lines still account for only 4.98% of all
lines. Classifying each line into one or more classes thus
appears to be more efﬁcient for meta-tagging than classifying each word. Table 2 lists the class distributions of the
lines from the 500 training headers.
The predicted tags for previous and next lines are also
good indicators of the class(es) to which a line belongs. For
instance, an abstract has consecutive lines uninterrupted by
lines of other classes, and title lines usually come before
author lines. Making use of such contextual information
among lines we feel will increase the line classiﬁcation performance.
We propose a third algorithm for processing the lines
predicted to contain chunks of information from multiple
classes. Since each chunk has consecutive words, we consider extracting metadata from the multi-class lines as the
problem of seeking the optimal chunk boundaries. Recognition of individual author names within multi-author lines
can also be considered as the problem of seeking the right
chunk boundary, in this case between the author names. For
example, does the line “Chungki Lee James E. Burns” refer to two authors “Chungki Lee” and “James E. Burns,”
two authors “Chungki Lee James” and “E. Burns,” or one
author “Chungki Lee James E. Burns”?
Based on the structural patterns of the document headers,

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

1:<title> Stochastic Interaction and Linear Logic +L+ </title>
2:<author> Patrick D. Lincoln John C. Mitchell Andre Scedrov
+L+ </author>
3: <abstract> Abstract +L+
4:We present stochastic interactive semantics for prepositional
linear +L+
...
22:<email>
jcm@cs.stanford.edu
</email>
<web>
http://theory.stanford.edu/people/jcm/home.html </web> <afﬁliation> Department of Computer Science, Stanford University,
</afﬁliation> <address> Stanford, CA 94305.</address> <note>
Supported in part +L+
23:by an NSF PYI Award, matching funds from Digital Equipment
Corporation, the Pow-ell Foundation, and Xerox Corporation;
and the Wallace F. and Lucille M. Davis Faculty +L+
24:Scholarship. +L+ </note>
25:<email>
andre@cis.upenn.edu
</email>
<web>http://www.cis.upenn.edu/ andre
</web>
<afﬁliation> Department of Mathematics, University of Pennsylvania,
</afﬁliation> <address> Philadelphia, PA 19104-6395. </address> <note> Partially supported by +L+
26:NSF Grants CCR-91-02753 and CCR-94-00907 and by ONR
Grant N00014-92-J-1916. Sce-drov is an American Mathematical
Society Centennial Research Fellow. +L+ </note>

Figure 1. Example 1 labeled document header
and metadata. Each line starts with the line
number.

we decompose the metadata extraction problem into two
sub-problems – (1) line classiﬁcation and (2) chunk identiﬁcation of multi-class and multi-author lines. Accurate line
classiﬁcation is a critical step, since it directly affects the
performance of the chunk identiﬁcation module.

3 Metadata Extraction Algorithm
This section describes two important aspects of our
work, SVM classiﬁcation and feature extraction. The metadata extraction algorithm is discussed in detail, together
with the corresponding ten-fold cross-validation result on
the 500 training headers. Performance is evaluated using
accuracy, precision, recall, and F measure.

3.1 Support Vector Machine Classiﬁcation
Support Vector Machine is well known for its generalization performance and ability in handling high dimension
data. Consider a two class classiﬁcation problem. Let (Ü½ ,
Ý½ ), ... ,(ÜÆ , ÝÆ ) be a two-class training dataset, with Ü
a training feature vector and their labels Ý
(-1, +1). The

Table 2. Class distribution among 10025 total
lines from 500 training header
Class No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Class Name
Title
Author
Afﬁliation
Address
Note
Email
Date
Abstract
Introduction
Phone
Keyword
Web
Degree
Pubnum
Page

Number of Lines
832
724
1065
629
526
336
182
5007
326
61
142
38
169
116
166

Percentage
8.3%
7.2%
10.6%
6.3%
5.2%
3.4%
1.8%
50.0%
3.3%
0.6%
1.4%
0.4%
1.7%
1.1%
1.7%

SVM attempts to ﬁnd an optimal separating hyperplane to
maximally separate two classes of training samples. The
corresponding decision function is called a classiﬁer. The
µ and it
kernel function of an SVM is written as ´
can be an inner product, Gaussian, polynomial, or any other
function that obeys Mercer’s condition [30, 6].
We choose the Gaussian kernel for the SVM and base
our experiment on the software SVM light [11]. We set the
parameter gamma (-g), the spread of the Gaussian kernel as
¼ ½, and all other parameters set by SVM light. We extend
the SVM to multi-class classiﬁers in the “One class versus
all others” approach, i.e., one class is positive and the remaining classes are negative.

ÃÜ Ü

3.2 Feature Extraction
Most of the previous work on information extraction
uses word-speciﬁc feature representations [27, 15, 29]. Recent research on the topic suggests that line-speciﬁc features
also could be useful [20].
We make use of both word and line-speciﬁc features to
represent our data. Each line is represented by a set of word
and line-speciﬁc features.
We design a rule-based, context-dependent word clustering method explained below for word-speciﬁc feature
generation, with the rules extracted from various domain
databases and text orthographic properties of words (e.g.
capitalization) [26]. Word clustering methods group similar words and use the cluster as a feature. Distributional
clustering methods have shown signiﬁcant dimensionality
reduction and accuracy improvement in text classiﬁcation

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

[2, 28, 8]. While distributional clustering needs to use labeled training data, our rule-based method relies on the
prior knowledge embedded in domain databases.
We collect the following databases to gather apriori
knowledge of the domain:

¯ Standard on-line dictionary of Linux system
¯ Bob Baldwin’s collection of 8441 ﬁrst names and
19613 last names
¯ Chinese last names
¯ USA state names and Canada province names
¯ USA city names
¯ Country names from the World Fact Book [1], and
¯ Month names and their abbreviations
We also construct domain databases, i.e., word lists from
training data for classes: afﬁliation, address, degree, pubnum, note, abstract, keyword, introduction, and phone.
Words and bigrams that appear frequently in the lines of
each class mentioned are selected to enter these word lists.
Frequency thresholding is used to deﬁne the list size [32].
The abstract class word list contains one word “abstract”
and the afﬁliation class list contains words shown in Table 3.
We then cluster words and bigrams based on their membership in the domain databases and their text orthographic
properties. The words and bigrams in the same cluster
are represented by a common feature, which we call wordspeciﬁc feature. For example, an author line “Chungki Lee
James E. Burns” is represented as “Cap1NonDictWord: :MayName: :MayName: :SingleCap: :MayName:”, after word clustering.
Such word clustering shows signiﬁcant improvement in
our experiment of classifying lines (details will be given in
another paper). A reason is that the word cluster statistics
give a more robust estimate than the original sparse word
statistics [2, 28].
We deﬁne the weight of a word-speciﬁc feature as the
number of times this feature appears in the sample (line).
The following is the list of line-speciﬁc features we believe to be useful for line classiﬁcation. In particular, feature
ClinePos is found to be very important in correct classiﬁcation of title lines.
CsenLen Number of the words the line contains.
ClinePos The position of the line, i.e., line number.
CDictWordNumPer The percentage of the dictionary words in
the line.
CNonDictWordNumPer The percentage of the non-dictionary
words in the line.
CCap1DictWordNumPer The percentage of the dictionary
words with ﬁrst letter capitalized in the line.
CCap1NonDictWordNumPer The percentage of the non-dict
words with ﬁrst letter capitalized in the line.

CdigitNumPer The percentage of the numbers in the line.

We also have a feature for representing the percentage
of the class-speciﬁc words in a line. CafﬁNumPer is the
percentage of the afﬁliation words in the line and CaddrNumPer, CdateNumPer, CdegreeNumPer, CphoneNumPer,
CpubNumPer, CnoteNumPer, and CpageNumPer are the
percentage of the address words, date words, degree words,
phone words, publication number words, note words, and
page number words, respectively. We assign weight to the
line-speciﬁc features according to their deﬁnition.
Table 3. Afﬁliation Class Word List
DF Value
325
221
111
77
47
39

Word
University
Department
Univ
Institute
Research
Sciences

DF Value
37
34
33
27
26
26

Word
Laboratory
Technology
Dept
Systems
School
Center

However, our experiments show that SVM doesn’t handle well the case when different features have very different ranges of values. For example, the feature “CsenLen”
could have a weight of 40, while the line-speciﬁc feature
CdictWordNumPer weight is over the range [0, 1]. Features with large scale may dominate the features with small
weight. Therefore, we use the
½ to normalize the feature weight and increase the classiﬁcation performance as
shown in the next section.

3.3 Line Classiﬁcation Algorithms
The following is a two-step algorithm for classifying text
lines into a single class or multiple classes. The two components are independent line classiﬁcation followed by contextual line classiﬁcation.
3.3.1 Independent line classiﬁcation
In the ﬁrst step, feature vectors are generated based on the
feature extraction methodology described in the previous
section. After removing the features with data frequency
¿, we get feature vectors with 1100 dimensions
values
on average for ten-fold cross validation. A feature vector is
labeled as class if the corresponding line contains words
belonging to class . Training feature vector set for class
is generated by collecting all the feature vectors with label
as positive samples and all the rest as negative; the same
procedure applies to all classes. Note that a feature vector
could have multiple labels and thus can belong to multiple
training feature vector sets. 15 classiﬁers are then trained

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

Table 4. Word-speciﬁc feature set
Feature
:email:
:url:
:singleCap:
:postcode:
:abstract:
:keyword:
:intro:
:phone:
:month:
:prep:
:degree:

Explanation
using regular expression match
using regular expression match
a capital letter like M or M.
such as PA, MI
abstract
key word, key words, keyword, keywords
introduction
tel, fax, telephone
a word in the month list
at, in, of
a word or bigram in the degree domain
word list
:pubnum:
a word or bigram in the publication
number domain word list
:notenum:
a word or bigram in the note domain
word list
:afﬁ:
a word or bigram in the afﬁliation
domain word list
:addr:
a word or bigram in the address domain
word list
:city:
a word or bigram in the city name list
:state:
a word or bigram in the state name list
:country:
a word or bigram in the country name
list
:mayName:
a word in one of the 3 name lists
:Cap1DictWord: a dictionary word with ﬁrst
letter capitalized
:DictWord:
small case dictionary word
:NonDictWord:
small case non dictionary word
:Dig[3]:
a number of three digits
The word-speciﬁc feature considers text orthographic properties,
e.g., BU-cs-93 is converted to :CapWord2-LowerWord2-Digs2:

on the 15 labeled feature vector sets. Test lines are classiﬁed into one or more classes if their feature vectors are
scored positive by the corresponding classiﬁer. This process is called independent line classiﬁcation (also shown in
Figure 2), since each line is classiﬁed independently.
Table 5 lists the ten-fold cross-validation results on the
training dataset for the independent line classiﬁcation algorithm. Figure 3 shows the F measure of independent line
classiﬁcation before and after normalization using ten-fold
cross-validation on 500 training headers. Due to space limitations, we are not able to report our results for precision,
recall, and accuracy. The effect of normalization is a signiﬁcant improvement in performance. Normalization is especially important in identifying the rare classes, such as
class 5 (note), 11 (keywords), and 12 (web). Consider class
5 “note” as an example, the positive note samples occupy
5.3% (53 out of 1001.5 averaged for each fold of ten-fold
cross validation) of all test samples. Without normalization,

Fmeasure
1
0.8
0.6
0.4
0.2
0

Unnormalized
Normalized
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

Figure 3. F measure of the independent line
classiﬁcation before and after normalization.
X axis - class number; Y axis - F measure.

tion predicted in the previous iteration. The procedure converges when the percentage of lines with new class labels is
lower than a threshold. The threshold value is set to 0.7%
in our experiments, and is chosen to be 5. Ramshaw et al
show the positive effect of a similar iterative algorithm on
transformation-based learning for rule-selection [25].
The contextual information we use for line classiﬁcation
if the previous ith closis encoded by the binary features
if the next ith closest line
est line belongs to class j and
belongs to class j, with ¾ ´½ µ and ¾ ´½ ½ µ. We
and
to be 0.5/0, instead of 1/0
found that choosing
achieves better line classiﬁcation performance, based on the
experiment on the training dataset. This is because the line
feature values are already normalized into the range [0, 1].
Choosing the midpoint of this range as the weight for up to
150 (15£10) contextual features is a type of normalization
and is found to be more effective.
Figure 4 shows the performance evaluated by the F
measure in each round of the iterative contextual line
classiﬁcation. As expected, the performance is stabilized
within the ﬁrst 10 iterations. It also shows that the ﬁrst
two rounds are responsible for most of the performance
improvement. This behavior suggests two iteration steps
can be used instead of waiting for absolute convergence.
Table 6 lists the results achieved for each of the 15 classes
when the iterative procedure converges. The small sample
sizes of the class – degree, note, phone, keyword, and
publication number – as shown in Table 6 may account
for their poor classiﬁcation performance. Seymore et al
report the same phenomenon on the class – degree, note
and publication number – using HMM model [27].

Æ

È

Figure 2. Overview of Line Classiﬁcation
Training Module.

the note classiﬁer classiﬁes all testing samples into non“note” classes. Thus, the recall for class 5 “note” is zero
and the precision is inﬁnite. Normalization appears to increase the importance of features in the class “note”, which
then enhances “note” samples for the “note” classiﬁer.
3.3.2 Iterative contextual line classiﬁcation
The second step makes use of the sequential information
among lines discussed in section 2 to improve the classiﬁlines
cation of each line. We encode the class labels of
before and after the current line as binary features and
concatenate them to the feature vector of line formed in
step one, independent line classiﬁcation. A contextual line
classiﬁer for each metatag is then trained based on these
labeled feature vectors with additional contextual information. Line feature vectors for testing are extended the same
way. Their neighbor lines’ class labels are those predicted
by the independent line classiﬁer. Test lines are then reclassiﬁed into one or more classes by the contextual line classiﬁers. This contextual line classiﬁcation is repeated such that
in each iteration, the feature vector of each line is extended
by incorporating the neighbor lines’ class label informa-

Ä

Æ

Ä

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

Æ
Æ

È

3.4 Extract metadata from multi-class lines
After classifying each line into one or more classes, we
now extract metadata from each multi-class line based on
the predicted class labels for this line. As discussed the
metadata extraction task from multi-class lines is turned
into the chunk identiﬁcation task. Chunk identiﬁcation of

Fmeasure
1

Title
Author
Affiliation
Address
Note
Email
Date
Abstract
Introduction
Phone
Keyword
Web
Degree
PubNO
Page

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

1

2

3

4

5

6

7

8

9

10

11

Figure 4. F measure in each round of the iterative contextual line classiﬁcation. X axis iteration round; Y axis - F measure.

Æ

Æ

an -class line is analogous to ﬁnding   ½ chunk boundaries in the line. Punctuation marks and spaces between
words are candidate chunk boundaries.
Table 7 shows that 86% of the multi-class lines in training data are two-class lines. We search for the optimal
chunk boundary which yields the maximum difference between the two chunks. Independent line classiﬁers are applied to calculate the difference between chunks.
Every punctuation mark and space can be a candidate
chunk boundary for two-class lines. We consider only punctuation marks as candidates if two or more punctuation
marks are used in the line; otherwise we try each punctuation mark and space. Assuming that each class has only
one chunk in the line, two-class chunk identiﬁcation is to
ﬁnd the optimal chunk boundary.
“The Ohio State University, Columbus, OH 43210-1277” is an
example of two-class line of afﬁliation and address. Each
comma is a candidate chunk boundary.
We call the afﬁliation classiﬁer as classiﬁer 1 and the
address classiﬁer as classiﬁer 2. The classiﬁers we use here
are the SVM line classiﬁers trained by single-class lines of
the training dataset. We consider each chunk as a short line.
Deﬁnitions:

È½ the classiﬁcation score of chunk P by classiﬁer 1;
È¾ the classiﬁcation score of chunk P by classiﬁer 2;
Æ½ the classiﬁcation score of chunk N by classiﬁer 1;
Æ¾ the classiﬁcation score of chunk N by classiﬁer 2;
È½¾ = È½ - È¾ ; Æ¾½ = Æ¾ - Æ½;
ÈÆ½ = È½ - Æ½; ÈÆ¾ = È¾ - Æ¾;

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

Table 5. Independent line classiﬁcation performance.
Class Name
Title
Author
Afﬁliation
Address
Note
Email
Date
Abstract
Introduction
Phone
Keyword
Web
Degree
Pubnum
Page

Precision
89.6%
94.2%
93.8%
93.9%
82.3%
97.6%
97.2%
96.1%
98.8%
93.8%
95.2%
100%
86.0%
91.7%
100.0%

Recall
88.9%
89.6%
84.2%
85.1%
45.4%
97.4%
89.4%
97.7%
96.0%
69.1%
55.2%
92.8%
52.8%
71.8%
100.0%

F measure
89.3%
91.8%
88.7%
89.3%
58.5%
97.5%
93.1%
96.9%
97.4%
79.5%
69.9%
96.3%
65.4%
80.5%
100.0%

Accuracy
98.2%
98.8%
97.7%
98.8%
96.6%
99.8%
99.8%
96.9%
99.8%
99.8%
99.3%
99.9%
98.9%
99.6%
100.0%

We choose the optimal chunk boundary as the punctuation mark or space yielding the maximal ½¾ £ ¾½ .
Chunk P is classiﬁed into class 1 if
¼, and (1)
½
£ ¾ ¼ or (2) ½ £ ¾ ¼ and
½
½
´
½
¾ µ, class 2 otherwise.
This two-class chunk identiﬁcation algorithm results in
an accuracy of 75.5% (160 out of 212 two-class lines from
training samples). Accuracy here is deﬁned as the percentage of the lines whose chunk boundaries are correctly predicted versus the total number of two-class lines. (This is
the lower boundary of the accuracy.)
¾ ) chunk identiﬁcation tasks
Many -class (
may be simpliﬁed to two-class chunk identiﬁcation tasks.
For instance, using the positions of email and URL in the
line, we may simplify the three-class chunk identiﬁcation
tasks as two-class chunk identiﬁcation tasks. The position of the email address in the following three-class line
“International Computer Science Institute, Berkeley, CA 94704.

ÈÆ ÈÆ
Ñ Ü ÈÆ

Æ

ÈÆ ÈÆ

ÈÆ

ÈÆ

È

Æ

ÈÆ

Æ

email: aberer@icsi.berkeley.edu. Supported by Schweizerische
Gesellschaft zur Forderung der Informatik und ihrer Anwendungen” is a natural chunk boundary between the other two

classes.
We are exploring more general multi-class chunk identiﬁcation techniques.

3.5 Recognize authors in the multi-author lines
We consider the author lines with less than 4 words as
single-author lines and the author lines with 4 or more
words as multi-author lines. We further deﬁne a multiauthor line where the authors are separated by spaces only

Table 6. Performance (%) of contextual line
classiﬁcation iteration algorithm when converges and the F measure increase than that
of the independent line classiﬁcation
Class Name

Precision

Recall

Title
Author
Afﬁliation
Address
Note
Email
Date
Abstract
Introduction
Phone
Keyword
Web
Degree
Pubnum
Page

93.9
97.3
96.4
93.6
86.4
98.9
97.2
98.5
100.0
98.3
96.7
100.0
91.4
97.3
100.0

95.0
91.4
90.3
86.7
65.6
94.0
89.5
99.2
96.4
62.3
79.5
92.8
80.5
65.5
100.0

F measure
(Increase)
94.5(5.2)
94.2(2.4)
93.3(4.5)
90.0(0.71)
74.6(16.0)
96.4(-1.1)
93.2(0.1)
98.8(1.9)
98.2(0.8)
76.2(-3.3)
87.2(17.3)
96.3(0.0)
85.6(20.1)
78.3(-2.2)
100.0(0.0)

Accuracy
99.1
99.2
98.6
98.8
97.6
99.8
99.8
98.8
99.9
99.7
99.7
99.9
99.3
99.6
100.0

author names. It is obvious that the spaces and punctuation marks between words are the candidate chunk boundaries. The problem now becomes classifying each space
or punctuation mark as chunk boundary or not. We consider only the punctuation mark in the line as the candidate
chunk boundary if there are two or more punctuation marks
in the line; otherwise, we examine each space and punctuation mark. The dictionary word “and” is considered as a
punctuation mark. The spaces next to a punctuation mark
are ignored.
We design the feature vector for each space and punctuation mark using both the raw features of the punctuation
mark itself such as “,” or “&”, and the contextual features
listed in Table 8. We also convert each word of the line into
Æ ÄÆ Ä
. Each element of the
a 5-tuple
5-tuple is deﬁned as follows.

Æ : ½ if the word is in the ﬁrst name list, ¼ otherwise.
ÄÆ : ½ if the word is in the last name list, ¼ otherwise.
Ä: ½, ¾ or ¼, indicates the word is of one letter, two letters,
or more than two letters, respectively.

: ½ if the word is capitalized, ¼ otherwise.
: ½ if the word is a dictionary word, ¼ otherwise.

Table 7. The distribution of the multi-class
lines in 500 training headers
N-Class
2
3
4
5

Number of Lines
212
33
4
1

Percentage
84.8%
13.2%
1.6%
0.4%

as space-separated multi-author line. Similarly, a multiauthor line where the authors are separated by punctuation marks is deﬁned as punctuation-separated multi-author
line.
We extract a total of 326 multi-author lines from the
training dataset as the dataset for our experiment on recognizing authors from the multi-author lines. Among the
326 multi-author lines, 227(69.6%) lines are punctuationseparated and 99(30.4%) are space-separated. Based on the
different characteristics punctuation-separated multi-author
lines and space-separated multi-author lines possess, we
choose the following different strategies for either case.
3.5.1 Chunk identiﬁcation in punctuation-separated
multi-author lines
As we discussed before, to recognize each name from the
multi-author lines is to identify chunk boundaries between

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

We use the attributes deﬁned in the above tuple to represent the contextual feature (8) in Table 8 in the converted
format. The motivation is that if the closest word to a
punctuation mark appears only on the ﬁrst name list, or
only on the last name list, it helps to classify if this punctuation mark is the right chunk boundary. For example,
if “Leonidas Fegaras, David Maier” satisﬁes this pattern
“[10010(First name)] [01011(Last name)], [10011(First
name)] [00010(Last name)]”, it will be reasonable to classify the comma as the right chunk boundary. However, the
big overlap between the ﬁrst name list and the last name list
makes such feature representation of each word ineffective.
We ﬁnd from the stepwise feature selection that the dominating features in classifying chunk boundary are the punctuation marks themselves. Therefore in implementation, we
design simple heuristic rules to make use of the punctuation
marks to extract each name from the punctuation-separated
multi-author line.
Table 9 lists the chunk identiﬁcation performance on
punctuation-separated multi-author lines. The evaluation is
based on the percentage of punctuation marks classiﬁed correctly.
3.5.2 Chunk identiﬁcation in space-separated multiauthor lines
Space-separated multi-author lines do not have any explicit information for chunk boundary recognition, unlike
punctuation-separated lines. The valid patterns for author

Table 8. Contextual features for each candidate chunk boundary in punctuationseparated multi-author line
No.
1
2
3
4
5
6
7
8

Feature
The number of total punctuation marks of the
same kind in the line
The position of this punctuation mark
The number of words before this punctuation mark
The number of words after this punctuation mark
The number of words between the previous
and the current punctuation mark
The number of words between the current
and the next punctuation mark
The ratio of the number of words before and after
this punctuation mark
The previous and next 5 words in converted
feature representation

Table 9. Chunk boundary identiﬁcation performance of punctuation-separated multiauthor lines
Accuracy
93.31

Precision
82.38

Recall
96.65

F measure
88.95

names are the source of information in this case. [Mary(Full
Name)] [Y.(Name Initial)], for instance, cannot be a valid
name.
The algorithm for extracting names from spaceseparated multi-author lines has four steps. Step 1, generate all potential name sequences for the space-separated
multi-author lines based on the valid patterns of names that
we deﬁne in Table 10. Step 2, design the feature vector for
each potential name sequence. We manually label each potential name sequence as ½ or  ½ by checking each name in
this sequence from the web. Step 3, train a SVM name sequence classiﬁer by the labeled training samples. Step 4, if
the test space-separated multi-author line has only one potential name sequence, it is the predicted name sequence.
Otherwise, classify each of its potential name sequences.
The name sequence with the highest score is predicted as
the correct name sequence.
For example, the line “Alan Fekete David Gupta Victor
Luchangco Nancy Lynch Alex Shvartsman” has three potential name sequences (Figure 5). We generate three reasonable sequences, with each name separated by ¥. The
“1” and ”-1” in front of each name sequence identiﬁes the
sequence as a positive sample or a negative sample. The
number at the beginning of each sequence is the classiﬁca-

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

Table 10. The valid patterns of a name. “ ”Full Name; “   ” - Full Name with hyphen,
e.g., Jon-hey; “Á ” - Name Initial; “×” - lower
case word
Pattern Class
1

Patterns
´
´

2
3
4
5

 µ
  µ´

,´

  µ´   µ
  µ´   µ

e.g., Yu-Chee Tseng
  µÁ , ´   µÁÁ , ´
´
e.g., Dhabaleswar K. Panda
Á , ÁÁ
e.g., C. L. Giles

Á´
´

  µÁÁÁ

 µ
  µ××

e.g., Th.P. van der Weide

tion score. The ﬁrst sequence achieves the highest score and
is predicted correctly.
The feature vector designed for each name sequence is
based on the following features. Let us assume Ä is a line
that contains Å names, Ò½ , Ò¾ through ÒÅ . For name Ò
(½
Å ) that has Æ words, we deﬁne the following
ﬁve features.

ÓÖÑ

the form of the Ø word of Ò ,
ÓÖÑ
¾
  Á × Ó . “Ó” - others.
È Ó× the position of the Ø word of Ò in the line.
Æ is equal to ½ if the Ø word of Ò is only in the ﬁrst name
list, ¼ otherwise.
ÄÆ is equal to ½ if the Ø word of Ò is only in the last name
list, ¼ otherwise.
ÆÓÒ
is equal to ½ if the Ø word of Ò is a non-dictionary
word, ¼ otherwise.

The feature ÓÖÑ has non-numerical values such as
“F”, “I” or “s”. We enumerate each of these name patterns
and assign these values as the weights of the corresponding
features.
We generated all the potential name sequences expanded
from the 99 space-separated name sequences as the name
sequence dataset. We achieve a classiﬁcation accuracy of
90.9% for ten-fold cross validation. Since we pick the potential sequence with the highest score for each unknown
name sequence, the accuracy is the ratio of the correct predictions to the total number of name sequences, which is 99
in this case.
Using SVM supervised learning to classify name sequences helps ﬁnd the implicit regularities that could have
been missed by the manual inspection. A regularity discovered from the training data is: hyphenated names such as
Jon-hey are not likely to be the last name.

Classiﬁcation Score
1.6398636
0.8996393
0.0061073704

Class label
1
-1
-1

Potential name sequences
Alan Fekete ¥ David Gupta ¥ Victor Luchangco ¥ Nancy Lynch ¥ Alex Shvartsman
Alan Fekete ¥ David Gupta ¥ Victor Luchangco Nancy ¥ Lynch Alex Shvartsman
Alan Fekete ¥ David Gupta Victor ¥ Luchangco Nancy ¥ Lynch Alex Shvartsman

Figure 5. Example of potential name sequences

4 Experimental results
Performance is evaluated by precision, recall, F measure,
and accuracy as described below.
Overall evaluation: The overall word classiﬁcation accuracy for the header is the percentage of the header words
that are tagged with the words’ true labels.
Class-speciﬁc evaluation: We deﬁne A as the number of
true positive samples predicted as positive, B as the number
of true positive samples predicted as negative, C as the number of true negative samples predicted as positive and D as
the number of true negative samples predicted as negative.
The sample may refer to the line in the line classiﬁcation
task and refer to the word when evaluating the ﬁnal metadata extraction performance.

È Ö × ÓÒ

·

ÙÖ Ý
Ñ ×ÙÖ

Ê

ÐÐ

·

·
· · ·
¾ÈÖ × ÓÒ£Ê ÐÐ
ÈÖ × ÓÒ·Ê ÐÐ

We apply the metadata extraction method discussed
earlier, with the parameters chosen from ten-fold crossvalidation on 500 training headers and 435 test headers. Our
method achieves an overall accuracy of 92.9%, better than
90.1% reported by Seymore et al. Table 11 compares our
method with the HMM method of multi-state L+D model
from Seymore et al on the classiﬁcation performance for
each class, except two functional classes “introduction” and
“end of page”. However, we are unable to obtain the classspeciﬁc accuracy method used by Seymore et al at the time
we submit this paper. Therefore, we also list class-speciﬁc
precision and recall for more effective evaluation.
We present below the Example 2 document header with
its true labels (Figure 6) and predicted labels (Figure 7) by
our metadata extraction algorithm. We also present the labels (Figure 8) our algorithm predicted for the Example 1
header shown in Figure 1 of section 2. The bold fonts indicate the predicted labels different from the true labels. Both
examples show the good performance of our algorithm on
labeling the single-class lines, and recognizing the individual authors from the multi-author lines. Line 6 in Figure 7
and line 22 in Figure 8 also show the good performance of
our two-class chunk identiﬁcation algorithm. The only dif-

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

Table 11. Comparison on the performance(%)
of metadata extraction using HMM and SVM
evaluated based on words. - Accuracy; È Precision and Ê - Recall
Class
Title
Author
Afﬁliation
Address
Note
Email
Date
Abstract
Phone
Keyword
Web
Degree
Pubnum

HMM(A)
98.3
93.2
89.4
84.1
84.6
86.9
93.0
98.4
94.9
98.5
41.7
81.2
64.2

SVM(A)
98.9
99.3
98.1
99.1
95.5
99.6
99.7
97.5
99.9
99.2
99.9
99.5
99.9

SVM(P)
94.1
96.1
92.2
94.9
88.9
90.8
84.0
91.1
93.8
96.9
79.5
80.5
92.2

SVM(R)
99.1
98.4
95.4
94.5
75.5
92.7
97.5
96.6
91.0
81.5
96.9
62.2
86.3

ference between our algorithm’s predictions and the original labels is line 7. Although we count this as a false prediction (in our evaluation), the original label for this line “note”
can be argued itself. The line contains two email addresses.
Therefore it could be labeled as email just as well. This kind
of uncertainty of labels is rare, though. Figure 8 shows the
direct impact the line classiﬁcation has on the chunk identiﬁcation performance. Wrongly classifying the ﬁve-class
line 22 in Figure 8 as the four-class line, causes the further incorrect chunk identiﬁcation. Wrongly classifying the
ﬁve-class line 25 as a single class line “note”, also disables
the further chunk identiﬁcation algorithm. A reason that
line 25 is wrongly classiﬁed as single-class line, is because
our contextual line classiﬁcation algorithm in Section 3.3.2
over weighs the contextual information of the “note” text
from line 22 to line 26.

5 Discussion and future work
This paper describes a classiﬁcation-based method using Support Vector Machines (SVM) for metadata extraction. These initial results achieve nominally better results
than Hidden Markov Model based methods. This occurs

1:<title> THE CORAL USER MANUAL +L+
2:A Tutorial Introduction to CORAL +L+ </title>
3:<author> Raghu Ramakrishnan Praveen Seshadri Divesh
Srivastava +L+ </author>
4:<author> S. Sudarshan +L+ </author>
5:<afﬁliation> Computer Sciences Department, +L+
6:University of Wisconsin-Madison,</afﬁliation><address> WI
53706, U.S.A. +L+ </address>
7:<note>The authors’ e-mail addresses are fraghu,divesh,
praveeng@cs.wisc.edu; sudarsha@research.att.com.+L+</note>

Figure 6. Example 2 document header with
the true labels.
1: chunk(1) - <title> - THE CORAL USER MANUAL
2: chunk(1) - <title> - A Tutorial Introduction to CORAL
3: chunk(1) - <author> - Raghu Ramakrishnan
chunk(2) - <author> - Praveen Seshadri
chunk(3) - <author> - Divesh Srivastava
4: chunk(1) - <author> - S. Sudarshan
5: chunk(1) - <afﬁliation> - Computer Sciences Department,
6: chunk(1) - <afﬁliation> - University of Wisconsin-Madison
chunk(2) - <address> - WI 53706, U.S.A.
7:
chunk(1) - <email> - The authors’ e-mail addresses are fraghu,divesh,praveeng@cs.wisc.edu;
sudarsha@research.att.com.

1:chunk(1) - <title> - Stochastic Interaction and Linear Logic
2:chunk(1) - <author> - Patrick D. Lincoln
chunk(2) - <author> - John C. Mitchell
chunk(3) - <author> - Andre Scedrov
3:chunk(1) - <abstract> - Abstract
4:chunk(1) - <abstract> - We present stochastic interactive
semantics for propositional linear
...
22:chunk(1) - <note> - jcm@cs.stanford.edu
chunk(2) - <web> - http://theory.stanford.edu/people/jcm/home.html)
chunk(3) - <afﬁliation> - Department of Computer Science, Stanford University
chunk(4) - <address> - Stanford, CA 94305. Supported in part
23:chunk(1) - <note> - by an NSF PYI Award , matching funds
from Digital Equipment Corporation, the Pow-ell Foundation,
and Xerox Corporation; and the Wallace F. and Lucille M. Dav is
Faculty
24:chunk(1) - <note> - Scholarship.
25:chunk(1) - <note> - andre@cis.upenn.edu
http://www.cis.upenn.edu/ andre Department of Mathematics,
University of Pennsylvania, Philadelphia, PA 19104-6395.
Partially supported by
26:chunk(1) - <note> - NSF Grants CCR-91-02 753 and
CCR-94-00907 and by ONR Grant N0001 4-92-J-1916. Scedrov
is an American Mathematical Society Centennial Research Fellow.

Figure 8. Example 1 document header labeled
by SVM metadata extraction algorithm.

Figure 7. Example 2 document header labeled
by SVM metadata extraction algorithm.

because we use apriori information of the structural pattern of the data, feature extraction based on domain speciﬁc
databases, an appropriate normalization technique, and an
iterative correction procedure. In addition, the method we
propose for extracting individual names from a list of author names has good performance. We believe that our results indicate a promising classiﬁcation-based method for
information extraction.
There are some aspects of our method that could still
be improved. The line classiﬁcation performance limits the
further multi-class line chunk identiﬁcation performance as
shown in Figure 8. We will add the functionality to correct
the errors caused by the line classiﬁcation algorithm. Some
chunks such as an integrated name may be broken into two
lines occasionally. In this case, the multi-class chunk algorithm may make the incorrect decision. We will combine
some of the consecutive lines of the same class to minimize
the corresponding errors. Currently we assume each line
has only one chunk for each class. This is not appropriate
even though it is rare for a class to have multiple chunks of

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

the same class in one line. It is worthwhile to explore more
general multi-class chunk identiﬁcation techniques.
In addition to extracting the taggable metadata from the
header part of the research papers, we will apply text summarization techniques, such as Zha’s [33], to extract the implicit metadata subject and description. This will have the
potential for generating a hierarchical metadata representation of the document. We also will intend to develop a robust and accurate wrapper for bibliographies and to deﬁne
and extract metatags for metadata as well as for equations
and ﬁgures.

6 Acknowledgments
We acknowledge Cheng Li and Guangyu Chen for useful
comments on the ﬁrst draft of the paper. We acknowledge
the valuable comments from reviewers. We would like to
acknowledge the support from NSF NSDL 0121679, partial
support from the Special Funds for Major State Basic Research Projects of China (project G1999032800), and partial support from Lockheed-Martin.

References
[1] The world factbook. http://www.cia.gov/cia/publications/
factbook/, 2002.
[2] L. D. Baker and A. K. McCallum. Distributional clustering
of words for text classiﬁcation. In W. B. Croft, A. Moffat,
C. J. van Rijsbergen, R. Wilkinson, and J. Zobel, editors,
Proc. of SIGIR-98, pages 96–103, 1998.
[3] T. Berners-Lee, J. Hendler, and O. Lassila. The semantic
web. Scientiﬁc American, 2001.
[4] T. Brody.
Celestial - Open Archives Gateway.
http://celestial.eprints.org/, 2002.
[5] H. L. Chieu and H. T. Ng. A maximum entropy approach
to information extraction from semi-structured and free text.
In Proc. 18th National Conference on Artiﬁcial Intelligence
(AAAI 2002), pages 786–791, 2002.
[6] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernel-Based Learning
Methods. Cambridge University Press, 2000.
[7] H. de Sompel and C. Lagoze. The Open Archives Initiative
Protocol for Metadata Harvesting, January 2001.
[8] I. Dhillon, S. Manella, and R. Kumar. A divisive information
theoretic feature clustering for text classiﬁcation. Machine
Learning Research (JMLR), 2002.
[9] S. Dumais, J. Platt, D. Heckerman, and M. Sahami. Inductive learning algorithms and representations for text categorization. In Proc. 7th International Conference on Information and Knowledge Management, pages 148–155, November 1998.
[10] H.Pasula, B.Marthi, B.Milch, S.Russell, and I.Shpitser.
Identity uncertainty and citation matching. In Proc. of the
Advances in Neural Information Processing Systems (NIPS),
2002.
[11] T. Joachims. Making large-scale Support Vector Machine learning practical. In B. Scholkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA, 1998.
[12] T. Joachims. A statistical learning model of text classiﬁcation with Support Vector Machines. In W. B. Croft, D. J.
Harper, D. H. Kraft, and J. Zobel, editors, Proc. SIGIR-01,
24th ACM International Conference on Research and Development in Information Retrieval, pages 128–136, 2001.
[13] A. Kent.
OAI Harvester Crawling Status.
http://www.mds.rmit.edu.au/ ajk/oai/interop/summary.htm,
2001.
[14] D. Knox. CITIDEL: Making resources available. In Proc.
7th Annual Conference on Innovation and Technology in
Computer Science Education, pages 225–225, 2002.
[15] T. Kudoh and Y. Matsumoto. Use of support vector learning
for chunk identiﬁcation. In Proc. of CoNLL-2000 and LLL2000, 2000.
[16] J. Lafferty, A. McCallum, and F. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289, 2001.
[17] S. Lawrence, C. L. Giles, and K. Bollacker. Digital Libraries and Autonomous Citation Indexing. IEEE Computer,
32(6):67–71, 1999.

Proceedings of the 2003 Joint Conference on Digital Libraries (JCDL’03)
0-7695-1939-3/03 $17.00 © 2003 IEEE

[18] X. Liu. Federating heterogeneous Digital Libraries by metadata harvesting. Ph.D. Dissertation, Old Dominion University, December 2002.
[19] C. C. Marshall. Making metadata: A study of metadata creation for a mixed physical-digital collection. In Proc. 3rd
ACM International Conference on Digital Libraries, pages
162–171, June 1998.
[20] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy Markov models for information extraction and segmentation. In Proc. 17th International Conf. on Machine
Learning, pages 591–598, 2000.
[21] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of internet portals with machine
learning. Information Retrieval Journal Volume 3, pages
127–163, 2000.
[22] P. Mcnamee and J. Mayﬁeld. Entity extraction without
language-speciﬁc resources. In D. Roth and A. van den
Bosch, editors, Proc. of CoNLL-2002, pages 183–186, 2002.
[23] A. Paepcke, C.-C. K. Chang, H. Garcia-Molina, and
T. Winograd. Interoperability for Digital Libraries worldwide. Communications of the ACM, 41(4):33–43, 1998.
[24] Y. Petinot, P. B. Teregowda, H. Han, C. L. Giles,
S. Lawrence, A. Rangaswamy, and N. Pal. eBizSearch: An
OAI-Compliant Digital Library for eBusiness. In Proc. the
ACM/IEEE Joint Conference on Digital Libraries (JCDL),
2003. In this proceeding.
[25] L. Ramshaw and M. Marcus.
Text chunking using
transformation-based learning.
In D. Yarovsky and
K. Church, editors, Proc. 3rd Workshop on Very Large Corpora, pages 82–94, 1995.
[26] P. Schone and D. Jurafsky. Knowlege-free induction of inﬂectional morphologies. In Proc. of the North American
chapter of the Association for Computational Linguistics
(NAACL-2001), 2001.
[27] K. Seymore, A. McCallum, and R. Rosenfeld. Learning hidden Markov model structure for information extraction. In
Proc. of AAAI 99 Workshop on Machine Learning for Information Extraction, pages 37–42, 1999.
[28] N. Slonim and N. Tishby. The power of word clusters for
text classiﬁcation. In Proc. 23rd European Colloquium on
Information Retrieval Research (ECIR), 2001.
[29] K. Takeuchi and N. Collier. Use of Support Vector Machines
in extended named entity. In D. Roth and A. van den Bosch,
editors, Proc. 6th Conference on Natural Language Learning (CoNLL-2002), 2002.
[30] V. Vapnik. Statistical Learning Theory. Springer Verlag,
New York, 1998.
[31] S. Weibel. The Dublin Core: A simple content description format for electronic resources. NFAIS Newsletter,
40(7):117–119, 1999.
[32] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In D. H. Fisher, editor,
Proc. ICML-97, 14th International Conference on Machine
Learning, pages 412–420, 1997.
[33] H. Zha. Generic summarization and keyphrase extraction
using mutual reinforcement principle and sentence clustering. In Proc. 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113–120, August 11-15 2002.

